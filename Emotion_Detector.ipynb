{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wsz6MExrUjJ"
      },
      "source": [
        "# CSC413 Final Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zNoZxVnrUjL"
      },
      "source": [
        "Alp Tarim, Mert Kaya, M. Rodin Karadeniz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-OMB20rUjN"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDA-NfBnrUjN"
      },
      "source": [
        "\n",
        "For this project, we will be comparing the industry standard Natural Language Processing (NLP) methods:\n",
        "\n",
        "- Recurrent Neural Network (RNN)\n",
        "- Long Short-Term Memory (LSTM)\n",
        "- Attention Network (with LSTM)\n",
        "- Transformers\n",
        "\n",
        "We will be comparing the computational cost, interpretability, and time complexities of each method by using them for the task of emotion detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54rscJ9-rUjO"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QmpGv6urUjO"
      },
      "source": [
        "### Setup Environment (if running in local environment)\n",
        "\n",
        "Put your environment setup here. You can use `!conda ...` for conda environment installation, or `!pip ...` for pip environment installations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QxG0X6drUjP",
        "outputId": "14af5565-acd1-40d2-8b2d-8faa50769299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "# code goes here\n",
        "!pip install tokenizers\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpSd2bfXrUjQ"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfJrdr-WrUjQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchtext\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import math\n",
        "\n",
        "from typing import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUtlvCHJrUjR"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsr7ogWKrUjS"
      },
      "source": [
        "#### GoEmotions Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MexifFNrUjS"
      },
      "source": [
        "##### Download GoEmotions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_HQJTQirUjS",
        "outputId": "75e9628b-ab47-433f-e150-bd689d7de97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=f31b2a3d8b8c164a62eb60b1469c7cd2c46cdc53e23439cc16aa44d002d3b4f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "--2022-04-23 02:11:06--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14174600 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_1.csv’\n",
            "\n",
            "goemotions_1.csv    100%[===================>]  13.52M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-23 02:11:06 (96.8 MB/s) - ‘data/full_dataset/goemotions_1.csv’ saved [14174600/14174600]\n",
            "\n",
            "--2022-04-23 02:11:06--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.128, 74.125.20.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14173154 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_2.csv’\n",
            "\n",
            "goemotions_2.csv    100%[===================>]  13.52M  89.4MB/s    in 0.2s    \n",
            "\n",
            "2022-04-23 02:11:07 (89.4 MB/s) - ‘data/full_dataset/goemotions_2.csv’ saved [14173154/14173154]\n",
            "\n",
            "--2022-04-23 02:11:07--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 74.125.20.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14395164 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_3.csv’\n",
            "\n",
            "goemotions_3.csv    100%[===================>]  13.73M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-23 02:11:07 (135 MB/s) - ‘data/full_dataset/goemotions_3.csv’ saved [14395164/14395164]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have wget installed with:\n",
        "!pip install wget\n",
        "\n",
        "!wget -nc -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
        "!wget -nc -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
        "!wget -nc -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHdiOHiXrUjV"
      },
      "source": [
        "##### Read GoEmotion CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap6XfyukrUjW"
      },
      "outputs": [],
      "source": [
        "EMOTIONS = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire',\n",
        "            'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "            'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness',\n",
        "            'surprise', 'neutral']\n",
        "\n",
        "EMOTION_GROUPS = {\n",
        "    \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
        "    \"disgust\": [\"disgust\"],\n",
        "    \"fear\": [\"fear\", \"nervousness\"],\n",
        "    \"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"],\n",
        "    \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n",
        "    \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"],\n",
        "    \"neutral\": [\"neutral\"],\n",
        "}\n",
        "\n",
        "\n",
        "def read_goemotions(full_dataset_path: str) -> pd.DataFrame:\n",
        "    used_cols = ['text'] + EMOTIONS\n",
        "\n",
        "    DATA_GOEMOTIONS1_UNPROCESSED = pd.read_csv(\n",
        "        f\"{full_dataset_path}/goemotions_1.csv\", usecols=used_cols, header=0)\n",
        "    DATA_GOEMOTIONS2_UNPROCESSED = pd.read_csv(\n",
        "        f\"{full_dataset_path}/goemotions_1.csv\", usecols=used_cols, header=0)\n",
        "    DATA_GOEMOTIONS3_UNPROCESSED = pd.read_csv(\n",
        "        f\"{full_dataset_path}/goemotions_1.csv\", usecols=used_cols, header=0)\n",
        "    return pd.concat([DATA_GOEMOTIONS1_UNPROCESSED,\n",
        "                      DATA_GOEMOTIONS2_UNPROCESSED,\n",
        "                      DATA_GOEMOTIONS3_UNPROCESSED])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLd5LhXxrUjW"
      },
      "source": [
        "##### Process Emotions From File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p11O-OcfrUjX"
      },
      "outputs": [],
      "source": [
        "def process_goemotions_dataset(table: pd.DataFrame) -> pd.DataFrame:\n",
        "    new_table = table.copy()\n",
        "\n",
        "    categories = EMOTION_GROUPS.keys()\n",
        "    category_indices = [i for i in range(len(categories))]\n",
        "    \n",
        "    for group in categories:\n",
        "        new_table[group] = new_table.loc[:, EMOTION_GROUPS[group]].sum(axis=1)\n",
        "        new_table[group] = (new_table[group] != 0).astype(int)\n",
        "    \n",
        "    sum_ = new_table.loc[:, categories].sum(axis=1)\n",
        "    # print(sum_ > 1 | sum_ == 0)\n",
        "    \n",
        "    new_table = new_table.drop(new_table[((sum_ > 1) | (sum_ == 0))].index)\n",
        "    \n",
        "    new_table['emotion'] = new_table.loc[:, categories] @ category_indices\n",
        "\n",
        "    emotion_not_category = [i for i in EMOTIONS if i not in categories]\n",
        "\n",
        "    return new_table.drop(emotion_not_category, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiaE0x5rUjX"
      },
      "source": [
        "#### Filter, Tokenize, Pad the GoEmotions Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qDqvcYrUjX"
      },
      "source": [
        "This part of the implementation is based from GoEmotions [GitHub](https://github.com/google-research/google-research/tree/master/goemotions), specifically the [`extract_words.py`](https://github.com/google-research/google-research/blob/master/goemotions/extract_words.py) file.\n",
        "\n",
        "THIS PART IS CURRENTLY USELESS IN THE PROJECT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0otTs0BcrUjY"
      },
      "outputs": [],
      "source": [
        "punct_chars = list((set(string.punctuation) | {\n",
        "  \"’\", \"‘\", \"–\", \"—\", \"~\", \"|\", \"“\", \"”\", \"…\", \"'\", \"`\", \"_\",\n",
        "  \"“\"\n",
        "}) - set([\"#\"]))\n",
        "punct_chars.sort()\n",
        "punctuation = \"\".join(punct_chars)\n",
        "replace = re.compile(\"[%s]\" % re.escape(punctuation))\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, float):\n",
        "        return []\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # eliminate urls\n",
        "    text = re.sub(r\"http\\S*|\\S*\\.com\\S*|\\S*www\\S*\", \" \", text)\n",
        "    # eliminate @mentions\n",
        "    text = re.sub(r\"\\s@\\S+\", \" \", text)\n",
        "    # substitute all other punctuation with whitespace\n",
        "    text = replace.sub(\" \", text)\n",
        "    # replace all whitespace with a single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # strip off spaces on either end\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    \n",
        "    cleaned_text = [w for w in words if len(w) > 2]\n",
        "    return \"\".join(cleaned_text)\n",
        "\n",
        "def generate_vocab(unprocessed):\n",
        "  # cleanup the text\n",
        "  processed = unprocessed.copy()\n",
        "  processed['text'] = processed['text'].apply(clean_text)\n",
        "\n",
        "  # generate a vocabulary\n",
        "  vocab = []\n",
        "  def add_to_vocab(text):\n",
        "    vocab.extend(text)\n",
        "\n",
        "  processed['text'].apply(add_to_vocab)\n",
        "\n",
        "  vocab = sorted(list(set(vocab)))\n",
        "  len(vocab)\n",
        "  vocab = dict((word, i) for i, word in enumerate(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2j98R4-rUjZ"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX7k7HCcrUjZ"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, Encoding, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
        "\n",
        "\n",
        "def train_tokenizer(data: pd.DataFrame) -> Tokenizer:\n",
        "    tokenizer = Tokenizer(models.Unigram())\n",
        "    tokenizer.normalizer = normalizers.BertNormalizer()\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "    tokenizer.post_processor = processors.ByteLevel()\n",
        "    tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "    trainer = trainers.UnigramTrainer(\n",
        "        show_progress=True,\n",
        "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
        "        special_tokens=[\"[PAD]\"]\n",
        "    )\n",
        "    tokenizer.train_from_iterator(data.text.tolist(), trainer=trainer)\n",
        "\n",
        "    tokenizer.enable_padding(\n",
        "        pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\")\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def load_tokenizer() -> Tokenizer:\n",
        "    return Tokenizer.from_file(\"data/tokenizer.json\")\n",
        "\n",
        "\n",
        "def save_tokenizer(tokenizer: Tokenizer):\n",
        "    tokenizer.save(\"data/tokenizer.json\")\n",
        "\n",
        "\n",
        "def convert_encoding_to_ids(encoded: List[Encoding]) -> np.ndarray:\n",
        "    return np.array([i.ids for i in encoded])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA2h17FdrUjZ"
      },
      "outputs": [],
      "source": [
        "def test_tokenizer():\n",
        "  unprocessed = read_goemotions(\"data/full_dataset\")\n",
        "  data = process_goemotions_dataset(unprocessed)\n",
        "  tokenizer = train_tokenizer(data)\n",
        "\n",
        "  encoded_lst = tokenizer.encode_batch([\"hello world 😘\", \"super\"])\n",
        "  encoded_ids = convert_encoding_to_ids(encoded_lst)\n",
        "  print(encoded_ids.shape)\n",
        "\n",
        "# test_tokenizer()\n",
        "# TODO X: Turn the tokenized text into a useable torch.tensor (will be used as input) (batch size x sequence length x embedding dimension)\n",
        "# TODO labels: Turn the parsed emotions into a useable torch.tensor (will be used as labels) (batch size x num emotions)\n",
        "# TODO Create utility for \"config\" like in assignments to store hyperparameters and \"dimensions\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1jyqox2rUja"
      },
      "source": [
        "### Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh0cASIorUja"
      },
      "outputs": [],
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def attr_dict_example_usage():\n",
        "  rnn_args_s = AttrDict()\n",
        "  args_dict = {\n",
        "      \"data_file_name\": \"pig_latin_small\",\n",
        "      \"cuda\": True,\n",
        "      \"nepochs\": 50,\n",
        "      \"checkpoint_dir\": \"checkpoints\",\n",
        "      \"learning_rate\": 0.005,\n",
        "      \"lr_decay\": 0.99,\n",
        "      \"early_stopping_patience\": 20,\n",
        "      \"batch_size\": 64,\n",
        "      \"hidden_size\": 32,\n",
        "      \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "      \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "      \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "  }\n",
        "  rnn_args_s.update(args_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pe6l30LrUja"
      },
      "source": [
        "## Recurrent Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UHgQwQVrUja"
      },
      "outputs": [],
      "source": [
        "class EmoRNN(nn.Module):\n",
        "    \"\"\" The Neural Network Model that involves RNN layers. Its architecture is as follows:\n",
        "    * Embedding Layer (one-hot dim x embedding dim)\n",
        "    * RNN Layer (embedding dim x hidden dim -> hidden dim*2 x hidden dim)\n",
        "    * Linear Layer (hidden_dim x output dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, hidden_size, seq_len, num_emotions, num_layers=1, nonlinearity='tanh', batch_first=True, bidirectional=True):\n",
        "        super(EmoRNN, self).__init__()\n",
        "\n",
        "        # important dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = num_emotions\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # setup the embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
        "\n",
        "        # setup the RNN\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=self.hidden_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            nonlinearity=nonlinearity,\n",
        "            bias=False,\n",
        "            batch_first=batch_first,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.D = 2 if bidirectional else 1\n",
        "\n",
        "        self.classifier = nn.Linear(\n",
        "            in_features=self.hidden_size * self.D * self.seq_len,\n",
        "            out_features=self.output_size\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: finish\n",
        "        out = self.embedding(input)\n",
        "        out, _ = self.rnn(out)\n",
        "        out = self.classifier(\n",
        "            out.reshape(-1, self.hidden_size*self.D*self.seq_len))\n",
        "        out = self.softmax(out)\n",
        "        # print(out.size())\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWf8XceiSm8W"
      },
      "source": [
        "# Long Short-Term Memory Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFVXgSBxS3K1"
      },
      "outputs": [],
      "source": [
        "class EmoLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, seq_len, num_emotions, num_layers=1, nonlinearity='tanh', batch_first=True, bidirectional=True):\n",
        "        super(EmoLSTM, self).__init__()\n",
        "\n",
        "        # important dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = num_emotions\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # setup the embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
        "\n",
        "        # setup the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.hidden_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bias=False,\n",
        "            batch_first=batch_first,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.D = 2 if bidirectional else 1\n",
        "\n",
        "        self.classifier = nn.Linear(\n",
        "            in_features=self.hidden_size * self.D * self.seq_len,\n",
        "            out_features=self.output_size\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.embedding(input)\n",
        "        out, _ = self.lstm(out)\n",
        "        out = self.classifier(\n",
        "            out.reshape(-1, self.hidden_size*self.D*self.seq_len))\n",
        "        out = self.softmax(out)\n",
        "        # print(out.size())\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56lTO0OSvjF"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertForSentenceClassification(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        ##### START YOUR CODE HERE #####\n",
        "        # Add a linear classifier that map BERTs [CLS] token representation to the unnormalized\n",
        "        # output probabilities for each class (logits).\n",
        "        # Notes: \n",
        "        #  * See the documentation for torch.nn.Linear\n",
        "        #  * You do not need to add a softmax, as this is included in the loss function\n",
        "        #  * The size of BERTs token representation can be accessed at config.hidden_size\n",
        "        #  * The number of output classes can be accessed at config.num_labels\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        ##### END YOUR CODE HERE #####\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, labels=None, **kwargs):\n",
        "        outputs = super().forward(**kwargs)\n",
        "        ##### START YOUR CODE HERE #####\n",
        "        # Pass BERTs [CLS] token representation to this new classifier to produce the logits.\n",
        "        # Notes:\n",
        "        #  * The [CLS] token representation can be accessed at outputs.pooler_output\n",
        "        logits = self.classifier(outputs.pooler_output)\n",
        "        ##### END YOUR CODE HERE #####\n",
        "        if labels is not None:\n",
        "            outputs = (logits, self.loss(logits, labels))\n",
        "        else:\n",
        "            outputs = (logits,)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "X2sHobf7UcxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_gVnpy13iqC"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "import time\n",
        "import datetime\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def get_optimizer_and_scheduler(model, total_steps, lr=2e-5, weight_decay=0.01):\n",
        "    # Apply weight decay to all parameters beside the biases or LayerNorm weights\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': weight_decay},\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        # Warmup learning rate for first 10% of training steps\n",
        "        num_warmup_steps=int(0.10 * total_steps), \n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, validation_dataloader):\n",
        "    # Use GPU, if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup optimizer and LR scheduler \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    optimizer, scheduler = get_optimizer_and_scheduler(\n",
        "        model, total_steps, lr=5e-5, weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    loss_values = []\n",
        "    eval_accs = []\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        t0 = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(train_dataloader, unit=\"batch\") as train_pbar:\n",
        "            for batch in train_pbar:\n",
        "                train_pbar.set_description(f\"Training (epoch {epoch + 1})\")\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                model.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # This will return the loss because we have provided the `labels`.\n",
        "                outputs = model(\n",
        "                    input_ids=b_input_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels\n",
        "                )\n",
        "                \n",
        "                # The call to `model` always returns a tuple, so we need to pull the \n",
        "                # loss value out of the tuple.\n",
        "                _, loss = outputs\n",
        "\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"  * Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  * Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "            \n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            \n",
        "            with torch.no_grad():        \n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                outputs = model(\n",
        "                    input_ids=b_input_ids, \n",
        "                    attention_mask=b_input_mask\n",
        "                )\n",
        "            \n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        avg_eval_acc = eval_accuracy/nb_eval_steps\n",
        "        print(\"  * Accuracy: {0:.2f}\".format(avg_eval_acc))\n",
        "        print(\"  * Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "        eval_accs.append(avg_eval_acc)\n",
        "    print(\"Training complete!\")\n",
        "    return loss_values, eval_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UJqm2QMSfqZ"
      },
      "source": [
        "# Pre-process and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hlCAB25CrUjb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def log_info(info, args=[], verbose=True):\n",
        "    if verbose:\n",
        "        print(info, *args)\n",
        "\n",
        "\n",
        "def get_d_labels(tokenizer: Tokenizer, data: pd.DataFrame, verbose=True):\n",
        "    log_info(\"Getting inputs and labels from dataset...\", verbose=verbose)\n",
        "    log_info(\"\\t getting batch from dataframe...\", verbose=verbose)\n",
        "\n",
        "    batch = data.text.tolist()\n",
        "\n",
        "    log_info(\"\\t encoding the batch...\", verbose=verbose)\n",
        "\n",
        "    encoded_lst = tokenizer.encode_batch(batch)\n",
        "\n",
        "    log_info(\"\\t converting Encoding objects to list of ids\", verbose=verbose)\n",
        "\n",
        "    encoded_ids = convert_encoding_to_ids(encoded_lst)\n",
        "\n",
        "    log_info(\"\\t getting labels from dataframe\", verbose=verbose)\n",
        "\n",
        "    labels = data.loc[:, EMOTION_GROUPS.keys()].to_numpy()\n",
        "    return encoded_ids, labels\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "  # TODO: move all the logs into function calls so the train function doesnt look ugly :)\n",
        "    log_info(\"Reading dataset...\", verbose=opts.verbose)\n",
        "    unprocessed = read_goemotions(\"data/full_dataset\")\n",
        "    log_info(\"Preprocess dataset...\", verbose=opts.verbose)\n",
        "    data = process_goemotions_dataset(unprocessed)\n",
        "\n",
        "    if opts.train_tokenizer:\n",
        "        log_info(\"Training tokenizer...\")\n",
        "        tokenizer = train_tokenizer(data)\n",
        "        save_tokenizer(tokenizer)\n",
        "    else:\n",
        "        log_info(\"Loading tokenizer...\")\n",
        "        tokenizer = load_tokenizer()\n",
        "\n",
        "    d, labels = get_d_labels(tokenizer, data)\n",
        "\n",
        "    dataset_size, seq_len = d.shape\n",
        "    vocab_size = tokenizer.get_vocab_size()\n",
        "    dataset_size_2, num_emotions = labels.shape\n",
        "    \n",
        "    d = torch.from_numpy(d)\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "    assert dataset_size == dataset_size_2\n",
        "\n",
        "    # model = EmoRNN(\n",
        "    #     vocab_size=vocab_size,\n",
        "    #     hidden_size=opts.hidden_size,\n",
        "    #     seq_len=seq_len,\n",
        "    #     num_emotions=num_emotions,\n",
        "    # )\n",
        "\n",
        "    model = EmoLSTM(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        seq_len=seq_len,\n",
        "        num_emotions=num_emotions,\n",
        "    )\n",
        "\n",
        "    # model = Net(\n",
        "    #     TEXT.vocab.vectors,\n",
        "    #     nhead=5,  # the number of heads in the multiheadattention models\n",
        "    #     dim_feedforward=50,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    #     num_layers=6,\n",
        "    #     dropout=0.0,\n",
        "    #     classifier_dropout=0.0,\n",
        "    # ).to(device)\n",
        "\n",
        "    num_train = int(dataset_size * 0.8)\n",
        "    num_validation = int((dataset_size-num_train) * 0.5)\n",
        "    num_test = dataset_size - num_train - num_validation\n",
        "\n",
        "    train_d, val_d, test_d = d[:num_train], \\\n",
        "        d[num_train:num_train + num_validation], \\\n",
        "        d[num_train+num_validation:]\n",
        "    \n",
        "    train_labels, val_labels, test_labels = labels[:num_train], \\\n",
        "        labels[num_train:num_train + num_validation], \\\n",
        "        labels[num_train+num_validation:]\n",
        "    \n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_d).type(\n",
        "        torch.LongTensor), torch.tensor(train_labels).type(torch.LongTensor))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=opts.learning_rate)\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    test_losses = []\n",
        "    \n",
        "    training_accuracies = []\n",
        "    validation_accuracies = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    try:\n",
        "      model.train()\n",
        "      for epoch in range(opts.nepochs):\n",
        "          print(\"----------------------------------------------------------------------------------\")\n",
        "          print(f\"Epoch: {epoch}\")\n",
        "          \n",
        "          running_loss = 0\n",
        "          model.train()\n",
        "          \n",
        "          for sentences, labels in train_loader:\n",
        "              # sentences, labels = sentences.to(device), labels.to(device)            \n",
        "              optimizer.zero_grad()\n",
        "              pred = model.forward(sentences)\n",
        "              labels = labels.type(torch.FloatTensor)\n",
        "              loss = criterion(pred, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss += loss.item()\n",
        "          else:\n",
        "              model.eval() \n",
        "              # sentences, labels = sentences.to(device), labels.to(device)\n",
        "              \n",
        "              with torch.no_grad():\n",
        "                  training_losses.append(running_loss/len(train_loader))\n",
        "                  # training_accuracies.append(\n",
        "                  #     torch.ceil(pred & labels).sum() / num_train)\n",
        "                  \n",
        "                  pred = model(val_d)\n",
        "                  val_labels = val_labels.type(torch.FloatTensor)\n",
        "                  validation_losses.append(criterion(pred, val_labels))\n",
        "                  # validation_accuracies.append(\n",
        "                  #     torch.ceil(pred & val_labels).sum() / num_validation)\n",
        "                  \n",
        "                  pred = model(test_d)\n",
        "                  test_labels = test_labels.type(torch.FloatTensor)\n",
        "                  test_losses.append(criterion(pred, test_labels))\n",
        "                  # validation_accuracies.append(\n",
        "                  #     torch.ceil(pred & test_labels).sum() / num_test)\n",
        "              print(f\"training loss: {training_losses[-1]}, validation loss: {validation_losses[-1]}, test loss: {test_losses[-1]}\")\n",
        "              # print(f\"training accuracy: {training_accuracies[-1]}, validation accuracy: {validation_accuracies[-1]}, training accuracy: {test_accuracies[-1]}\")\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Exiting early from training.\")\n",
        "\n",
        "    losses = [training_losses, validation_losses, test_losses]\n",
        "    accuracies = [training_accuracies, validation_accuracies, test_accuracies]\n",
        "    return model, losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "n67vgThOrUjd",
        "outputId": "f4ca9f78-21d8-47aa-b69f-f3eb4f3e7f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset...\n",
            "Preprocess dataset...\n",
            "Training tokenizer...\n",
            "Getting inputs and labels from dataset...\n",
            "\t getting batch from dataframe...\n",
            "\t encoding the batch...\n",
            "\t converting Encoding objects to list of ids\n",
            "\t getting labels from dataframe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "Epoch: 0\n",
            "training loss: 1.770966266055365, validation loss: 1.761897325515747, test loss: 1.7666378021240234\n",
            "----------------------------------------------------------------------------------\n",
            "Epoch: 1\n",
            "training loss: 1.7619895000715513, validation loss: 1.7528631687164307, test loss: 1.7567628622055054\n",
            "----------------------------------------------------------------------------------\n",
            "Epoch: 2\n"
          ]
        }
      ],
      "source": [
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"train_tokenizer\": True,\n",
        "    \"hidden_size\": 16,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"nepochs\": 20,\n",
        "    \"verbose\": True\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "model, losses, accuracies = train(rnn_args_s)\n",
        "print(losses)\n",
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.plot(losses[0], label = 'training loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "x0-ul5ruUA8s",
        "outputId": "19247f72-0754-40dd-f2eb-2336afef0a19"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Emotion Detector.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f88f306bc62a62c93cabf63d204394d17623b24d28dfa96873d8dc1f24713453"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('pythonProject')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}